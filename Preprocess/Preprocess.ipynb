{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### импортируем все нужное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from redditscore.tokenizer_rus import CrazyTokenizer as RusCrazyTokenizer\n",
    "except:\n",
    "    print('положите в папку библы redditscore файл tokenizer_rus.py (рядом с обычным tokenizer.py)')\n",
    "\n",
    "from redditscore.tokenizer import CrazyTokenizer as EngCrazyTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расширяем список стопслов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('russian')\n",
    "stops.extend(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "badwords = [\n",
    "'я', 'а', 'да', 'но', 'тебе', 'мне', 'ты', 'и', 'у', 'на', 'ага',\n",
    "'так', 'там', 'какие', 'который', 'какая', 'туда', 'давай', 'короче', 'кажется', 'вообще',\n",
    "'ну', 'чет', 'неа', 'свои', 'наше', 'хотя', 'такое', 'например', 'кароч', 'как-то',\n",
    "'нам', 'хм', 'всем', 'нет', 'да', 'оно', 'своем', 'про', 'вы', 'тд', 'тп', 'т.д', 'т.п',\n",
    "'вся', 'вам', 'это', 'эта', 'эти', 'этот', 'прям', 'либо', 'как', 'мы',\n",
    "'просто', 'блин', 'очень', 'самые', 'твоем', 'ваша', 'кстати', 'вроде', 'типа', 'пока', 'ок',\n",
    "    '°', '͜ʖ', '͡', 'ツ', 'ʕ•ᴥ•ʔ'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops.extend(badwords)\n",
    "stops.remove('не')\n",
    "stops.remove('нет')\n",
    "stops.remove('есть')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'я', 'а', 'да', 'но', 'тебе', 'мне', 'ты', 'и', 'у', 'на', 'ага', 'так', 'там', 'какие', 'который', 'какая', 'туда', 'давай', 'короче', 'кажется', 'вообще', 'ну', 'чет', 'неа', 'свои', 'наше', 'хотя', 'такое', 'например', 'кароч', 'как-то', 'нам', 'хм', 'всем', 'нет', 'да', 'оно', 'своем', 'про', 'вы', 'тд', 'тп', 'т.д', 'т.п', 'вся', 'вам', 'это', 'эта', 'эти', 'этот', 'прям', 'либо', 'как', 'мы', 'просто', 'блин', 'очень', 'самые', 'твоем', 'ваша', 'кстати', 'вроде', 'типа', 'пока', 'ок', '°', '͜ʖ', '͡', 'ツ', 'ʕ•ᴥ•ʔ']\n"
     ]
    }
   ],
   "source": [
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расширяем список мер\n",
    "\n",
    "чтоб не лезть руками в код спейси делаем чуть более костыльно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['km', 'km²', 'km³', 'm', 'm²', 'm³', 'dm', 'dm²', 'dm³', 'cm', 'cm²', 'cm³', 'mm', 'mm²', 'mm³', 'ha', 'µm', 'nm', 'yd', 'in', 'ft', 'kg', 'g', 'mg', 'µg', 't', 'lb', 'oz', 'm/s', 'km/h', 'kmh', 'mph', 'hPa', 'Pa', 'mbar', 'mb', 'MB', 'kb', 'KB', 'gb', 'GB', 'tb', 'TB', 'T', 'G', 'M', 'K', '%', 'км', 'км²', 'км³', 'м²', 'м³', 'дм', 'дм²', 'дм³', 'см', 'см²', 'см³', 'мм', 'мм²', 'мм³', 'нм', 'кг', 'г', 'мг', 'м/с', 'км/ч', 'кПа', 'Па', 'мбар', 'Кб', 'КБ', 'кб', 'Мб', 'МБ', 'мб', 'Гб', 'ГБ', 'гб', 'Тб', 'ТБ', 'тбكم', 'كم²', 'كم³', 'م', 'م²', 'م³', 'سم', 'سم²', 'سم³', 'مم', 'مم²', 'مم³', 'كم', 'غرام', 'جرام', 'جم', 'كغ', 'ملغ', 'كوب', 'اكواب', 'мкг', 'л', 'мл', 'гр']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.char_classes import UNITS\n",
    "units = UNITS.split('|')\n",
    "for u in 'кг|г|мг|мкг|л|мл|гр'.split('|'):\n",
    "    if u not in units:\n",
    "        units.append(u)\n",
    "        \n",
    "print(units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Открываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/task2_ru_training.tsv', 'r', encoding='utf-8') as f:\n",
    "    df = pd.read_csv(f, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>760402871867367424</td>\n",
       "      <td>Настало время для ингаляторов. Дружок, Сальбут...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1035908416869462016</td>\n",
       "      <td>15) На прошлой зимней олимпиаде большинство лы...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1089839736427032577</td>\n",
       "      <td>Не соглашусь с заменой ЗОК на метопролол в так...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>779671488748224513</td>\n",
       "      <td>@di2m1 мезим Смекта Если отравление, то лоперамид</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>738309299756240897</td>\n",
       "      <td>Уберите микроволновки и имодиум  Действуют соу...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                              tweet  \\\n",
       "0   760402871867367424  Настало время для ингаляторов. Дружок, Сальбут...   \n",
       "1  1035908416869462016  15) На прошлой зимней олимпиаде большинство лы...   \n",
       "2  1089839736427032577  Не соглашусь с заменой ЗОК на метопролол в так...   \n",
       "3   779671488748224513  @di2m1 мезим Смекта Если отравление, то лоперамид   \n",
       "4   738309299756240897  Уберите микроволновки и имодиум  Действуют соу...   \n",
       "\n",
       "   class  \n",
       "0      0  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### выбираем тестовые твиты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [df['tweet'][8],\n",
    "             df['tweet'][9], # дефисы\n",
    "             df['tweet'][19],\n",
    "             df['tweet'][268], # дефисы, капс\n",
    "             \n",
    "             df['tweet'][273], # латиница, нет пробелов вокруг скобки\n",
    "             \n",
    "             df['tweet'][10], # 0,5г ; р/д\n",
    "             df['tweet'][20], # 125 мгк 2 раза ; Вентолин 100 мгк\n",
    "            \n",
    "             df['tweet'][2368], # нет пробела между предложениями\n",
    "             \n",
    "             df['tweet'][14], # кавычки, юзернейм, эмоджи\n",
    "             df['tweet'][18], # хештеги, юзернейм\n",
    "             df['tweet'][25], # смайл, юзернейм, эмоджи\n",
    "             df['tweet'][116], # эмоджи\n",
    "             df['tweet'][270], # смайл\n",
    "             \n",
    "             df['tweet'][232] # сокращенная ссылка\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tweet in test_data:\n",
    "#    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настраиваем токенизатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "тут версия на улучшенном русском спейси [отсюда](https://github.com/aatimofeev/spacy_russian_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_rus = RusCrazyTokenizer(lowercase=True, # к нижнему регистру\n",
    "                           keepcaps=True, # все кроме all капс\n",
    "                           urls='ЮРЛ', # удаляем ссылки\n",
    "                           twitter_handles='ЮЗЕРНЕЙМ', # заменяем все юзернеймы\n",
    "                           hashtags='ХЕШТЕГ', # пытаемся делить хештеги на слова (тут надо свой частотник закинуть)\n",
    "                           pos_emojis=True, neg_emojis=True, neutral_emojis=True, # меняем эмоджи на плейсхолдеры (кастомизировать)\n",
    "                           normalize=3, # удаляем больше 3 одинаковых знаков подряд\n",
    "                           remove_punct=True, # удаляем пунктуацию\n",
    "                           remove_breaks=True, # удаляем переносы строк\n",
    "                           ignore_stopwords=stops, # удаляет стоп слова (разобраться почему падает на кастомном списке)\n",
    "                           numbers = False,\n",
    "                           extra_patterns = False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "это на оригинальном английском (если не запускается русская)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_eng = EngCrazyTokenizer(lowercase=True, # к нижнему регистру\n",
    "                           keepcaps=True, # все кроме all капс\n",
    "                           urls='ЮРЛ', # удаляем ссылки\n",
    "                           twitter_handles='ЮЗЕРНЕЙМ', # заменяем все юзернеймы\n",
    "                           hashtags='ХЕШТЕГ', # пытаемся делить хештеги на слова (тут надо свой частотник закинуть)\n",
    "                           pos_emojis=True, neg_emojis=True, neutral_emojis=True, # меняем эмоджи на плейсхолдеры (кастомизировать)\n",
    "                           normalize=3, # удаляем больше 3 одинаковых знаков подряд\n",
    "                           remove_punct=True, # удаляем пунктуацию\n",
    "                           remove_breaks=True, # удаляем переносы строк\n",
    "                           ignore_stopwords=stops, # удаляет стоп слова (разобраться почему падает на кастомном списке)\n",
    "                           numbers = False,\n",
    "                           extra_patterns = False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_rus = list(tokenizer_rus._nlp.vocab.strings)\n",
    "#vocab_eng = list(tokenizer_eng._nlp.vocab.strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавляем еще несколько удалений и замен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(tweet, log = False, lang = 'eng') -> str:\n",
    "    tweet = re.sub('(\\w)\\.([А-ЯЁ])', r'\\1. \\2', tweet) # ставим пробелы\n",
    "    tweet = re.sub('-', ' ', tweet) # удаляем меняем деффисы на пробел\n",
    "    if log:\n",
    "        #print('!до токенизации')\n",
    "        print(tweet)\n",
    "    # токенизируем (см. удаления и замены выше)\n",
    "    if lang == 'rus':\n",
    "        tokens = tokenizer_rus.tokenize(tweet)\n",
    "    elif lang == 'eng':\n",
    "        tokens = tokenizer_eng.tokenize(tweet)\n",
    "    else:\n",
    "        raise KeyError('ты долбаеб? языка {} нет'.format(lang))\n",
    "        \n",
    "    if log:\n",
    "        #print('!после токенизации')\n",
    "        print(tokens)\n",
    "        \n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('\\d+', token):\n",
    "            try:\n",
    "                x = re.findall('[^\\d.,\\s]+', token)[0]\n",
    "                if x in units:\n",
    "                    token = 'NUM MEASURE'\n",
    "                else:\n",
    "                    continue\n",
    "            except:\n",
    "                token = 'NUM'\n",
    "        if re.search('[!\"#$%&\\'()*+/:;<=>?@[\\]^_`{|}~]', token): #удаляем оставшиеся символы\n",
    "            token = re.sub('[!\"#$%&\\'()*+/:;<=>?@[\\]^_`{|}~]', '', token)\n",
    "        \n",
    "        if token in units:\n",
    "            token = 'MEASURE'\n",
    "        \n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "            \n",
    "        new_tokens.append(token)\n",
    "            \n",
    "    if log:\n",
    "        #print('!после обработки каждого токена')\n",
    "        print(new_tokens)\n",
    "        #print('!output')\n",
    "    \n",
    "    return ' '.join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все эти слюнтяи, утверждающие, будто суицид-удел слабых людей, найдут тысячу причин не глотать горсть прозака.Им просто не хватает смелости.\n",
      "слюнтяи утверждающие суицид удел слабых людей найдут тысячу причин не глотать горсть прозака не хватает смелости \n",
      "\n",
      "Это вам не лыжи,сальбутамол не поможет. Россия - Норвегия 5:1\n",
      "не лыжи сальбутамол не поможет россия норвегия \n",
      "\n",
      "Если оланзапин мне не поможет, тогда мне назначат галоперидол. Не хочу докатываться до галоперидола. Надо что-то делать с этой проклятой тревожностью.\n",
      "оланзапин не поможет назначат галоперидол не хочу докатываться галоперидола делать проклятой тревожностью \n",
      "\n",
      "из-за флуоксетина катастрофически не хочется что-либо есть, но почему-то БЛИНЫ ИЗ ТЕРЕМКА в меня влезают всегда  Теремок, спасибо что живая\n",
      "флуоксетина катастрофически не хочется есть почему БЛИНЫ ТЕРЕМКА влезают теремок спасибо живая \n",
      "\n",
      "В моей голове Хенвон любит lil peep; перкосет, молли, перпл дранк(это все наркота, если кто не в курсе); организует нелегальные вечеринки.\n",
      "моей голове хенвон любит lil peep перкосет молли перпл дранк наркота не курсе организует нелегальные вечеринки \n",
      "\n",
      "Ципрофлоксацин 0,5г 2 р/д как смысл жизни\n",
      "ципрофлоксацин NUM NUM MEASURE NUM смысл жизни \n",
      "\n",
      "Ветеринар прописал ингаляции Фликсотидом 125 мгк 2 раза в сутки и еще прописал Вентолин 100 мгк на случай приступов.\n",
      "ветеринар прописал ингаляции фликсотидом NUM мгк NUM раза сутки прописал вентолин NUM мгк случай приступов \n",
      "\n",
      "За день выпила 12 таблеток алпразолама.Чувствую себя так расслабленно.Ох,как хорошо.Даже днем часа 3 спала.\n",
      "день выпила NUM таблеток алпразолама чувствую расслабленно ох днем часа NUM спала \n",
      "\n",
      "@KissMyBlacklist а не лечатся ли их футболисты от астмы, синдрома дефицита внимания и т.п. столь любимыми у нероссийских спортсменов \"законными\" амфетамином и декстроамфетамином?  а?🤨\n",
      "ЮЗЕРНЕЙМ не лечатся футболисты астмы синдрома дефицита внимания столь любимыми нероссийских спортсменов законными амфетамином декстроамфетамином \n",
      "\n",
      "@Vremya_Pokazhet  Я бы порекомендовал всем сотрудникам wada выпить мельдония, вдохнуть сальбутамола и попробовать приблизиться к результату любого медалиста! #wada #вада #крушельницкий #Пхенчхан2018\n",
      "ЮЗЕРНЕЙМ порекомендовал сотрудникам wada выпить мельдония вдохнуть сальбутамола попробовать приблизиться результату любого медалиста ХЕШТЕГ вада крушельницкий \n",
      "\n",
      "@northgender Питер 👌 Мем ещё в том, что тот же сероквель пару недель ждали, а потом выяснилось, что его нет нигде, и по заказу просто не позвонили :) Вот ща опять ждём квентиапин Уже раз четвёртый  + дохера аптек обшарили\n",
      "ЮЗЕРНЕЙМ питер POSEMOJI мем ещё сероквель пару недель ждали выяснилось нигде заказу не позвонили ща ждём квентиапин четвёртый дохера аптек обшарили \n",
      "\n",
      "зодак,  цитрин, фексадин, кларитин, ксизал, супрастин, лоратадин,  фликсоназе - ничего не помогает 🤧🌿🍃☘️ сейчас попробую эриус\n",
      "зодак цитрин фексадин кларитин ксизал супрастин лоратадин фликсоназе не помогает попробую эриус \n",
      "\n",
      "Каждый раз с новыми таблетками:  Я: О, тебя зовут Паксил. Надеюсь ты оправдаешь своё название и дашь мне пак сил Паксил: Держи трясучечку  ( ͡° ͜ʖ ͡°)\n",
      "каждый новыми таблетками зовут паксил надеюсь оправдаешь своё название дашь пак сил паксил держи трясучечку \n",
      "\n",
      "Доброе утро коллеги ! Кофеин состоит из углерода, водорода, азота и кислорода — так же, как и кокаин, талидомид, нейлон, тротил и героин.  Приятного кофепития. https://t.co/1XmD9manJj\n",
      "доброе утро коллеги кофеин состоит углерода водорода азота кислорода кокаин талидомид нейлон тротил героин приятного кофепития ЮРЛ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for tweet in test_data:\n",
    "    print(tweet)\n",
    "    tweet = process(tweet, log=False, lang='rus')\n",
    "    res.append(tweet)\n",
    "    print(tweet, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['слюнтяи утверждающие суицид удел слабых людей найдут тысячу причин не глотать горсть прозака не хватает смелости',\n",
       " 'не лыжи сальбутамол не поможет россия норвегия',\n",
       " 'оланзапин не поможет назначат галоперидол не хочу докатываться галоперидола делать проклятой тревожностью',\n",
       " 'флуоксетина катастрофически не хочется есть почему БЛИНЫ ТЕРЕМКА влезают теремок спасибо живая',\n",
       " 'моей голове хенвон любит lil peep перкосет молли перпл дранк наркота не курсе организует нелегальные вечеринки',\n",
       " 'ципрофлоксацин NUM NUM MEASURE NUM смысл жизни',\n",
       " 'ветеринар прописал ингаляции фликсотидом NUM мгк NUM раза сутки прописал вентолин NUM мгк случай приступов',\n",
       " 'день выпила NUM таблеток алпразолама чувствую расслабленно ох днем часа NUM спала',\n",
       " 'ЮЗЕРНЕЙМ не лечатся футболисты астмы синдрома дефицита внимания столь любимыми нероссийских спортсменов законными амфетамином декстроамфетамином',\n",
       " 'ЮЗЕРНЕЙМ порекомендовал сотрудникам wada выпить мельдония вдохнуть сальбутамола попробовать приблизиться результату любого медалиста ХЕШТЕГ вада крушельницкий',\n",
       " 'ЮЗЕРНЕЙМ питер POSEMOJI мем ещё сероквель пару недель ждали выяснилось нигде заказу не позвонили ща ждём квентиапин четвёртый дохера аптек обшарили',\n",
       " 'зодак цитрин фексадин кларитин ксизал супрастин лоратадин фликсоназе не помогает попробую эриус',\n",
       " 'каждый новыми таблетками зовут паксил надеюсь оправдаешь своё название дашь пак сил паксил держи трясучечку',\n",
       " 'доброе утро коллеги кофеин состоит углерода водорода азота кислорода кокаин талидомид нейлон тротил героин приятного кофепития ЮРЛ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## тут можно тестировать конкретные случаи и писать о  траблах ане"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = 'я вдолбил 100г мефа и 20мл пива 50л колы и 18кг моркови а еще потом занюхал 18мг герыча'\n",
    "log = False\n",
    "lang = 'rus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizer = EngCrazyTokenizer(lowercase=True, # к нижнему регистру\n",
    "                           keepcaps=True, # все кроме all капс\n",
    "                           urls='ЮРЛ', # удаляем ссылки\n",
    "                           twitter_handles='ЮЗЕРНЕЙМ', # заменяем все юзернеймы\n",
    "                           hashtags='ХЕШТЕГ', # пытаемся делить хештеги на слова (тут надо свой частотник закинуть)\n",
    "                           pos_emojis=False, neg_emojis=False, neutral_emojis=False, # меняем эмоджи на плейсхолдеры (кастомизировать)\n",
    "                           normalize=3, # удаляем больше 3 одинаковых знаков подряд\n",
    "                           remove_punct=True, # удаляем пунктуацию\n",
    "                           remove_breaks=True, # удаляем переносы строк\n",
    "                           ignore_stopwords=False, # удаляет стоп слова (разобраться почему падает на кастомном списке)\n",
    "                           numbers = False,\n",
    "                           extra_patterns = False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before  : я вдолбил 100г мефа и 20мл пива 50л колы и 18кг моркови а еще потом занюхал 18мг герыча\n",
      "after   : вдолбил NUM MEASURE мефа NUM MEASURE пива NUM MEASURE колы NUM MEASURE моркови занюхал NUM MEASURE герыча\n",
      "del/repl: я, 100, г, и, 20мл, 50л, и, 18, кг, а, еще, потом, 18, мг\n"
     ]
    }
   ],
   "source": [
    "print('before  :', test_phrase)\n",
    "res_phrase = process(test_phrase, log = log, lang = lang)\n",
    "print('after   :',res_phrase)\n",
    "\n",
    "print('del/repl:', ', '.join([w for w in test_tokenizer.tokenize(test_phrase) if w not in res_phrase.split(' ')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "слюнтяи утверждающие суицид удел слабых людей найдут тысячу причин не глотать горсть прозака не хватает смелости\n",
      "слюнтя утвержда суицид удел слаб люд найдут тысяч причин не глота горст прозак не хвата смелост \n",
      "\n",
      "не лыжи сальбутамол не поможет россия норвегия\n",
      "не лыж сальбутамол не поможет росс норвег \n",
      "\n",
      "оланзапин не поможет назначат галоперидол не хочу докатываться галоперидола делать проклятой тревожностью\n",
      "оланзапин не поможет назначат галоперидол не хоч докатыва галоперидол дела проклят тревожн \n",
      "\n",
      "флуоксетина катастрофически не хочется есть почему БЛИНЫ ТЕРЕМКА влезают теремок спасибо живая\n",
      "флуоксетин катастрофическ не хочет ест поч блин теремк влеза теремок спасиб жив \n",
      "\n",
      "моей голове хенвон любит lil peep перкосет молли перпл дранк наркота не курсе организует нелегальные вечеринки\n",
      "мо голов хенвон люб lil peep перкосет молл перпл дранк наркот не курс организ нелегальн вечеринк \n",
      "\n",
      "ципрофлоксацин NUM NUM MEASURE NUM смысл жизни\n",
      "ципрофлоксацин NUM NUM MEASURE NUM смысл жизн \n",
      "\n",
      "ветеринар прописал ингаляции фликсотидом NUM мгк NUM раза сутки прописал вентолин NUM мгк случай приступов\n",
      "ветеринар прописа ингаляц фликсотид NUM мгк NUM раз сутк прописа вентолин NUM мгк случа приступ \n",
      "\n",
      "день выпила NUM таблеток алпразолама чувствую расслабленно ох днем часа NUM спала\n",
      "ден вып NUM таблеток алпразолам чувств расслаблен ох днем час NUM спал \n",
      "\n",
      "ЮЗЕРНЕЙМ не лечатся футболисты астмы синдрома дефицита внимания столь любимыми нероссийских спортсменов законными амфетамином декстроамфетамином\n",
      "юзернейм не лечат футболист астм синдром дефицит вниман стол любим нероссийск спортсмен закон амфетамин декстроамфетамин \n",
      "\n",
      "ЮЗЕРНЕЙМ порекомендовал сотрудникам wada выпить мельдония вдохнуть сальбутамола попробовать приблизиться результату любого медалиста ХЕШТЕГ вада крушельницкий\n",
      "юзернейм порекомендова сотрудник wad вып мельдон вдохнут сальбутамол попробова приблиз результат люб медалист хештег вад крушельницк \n",
      "\n",
      "ЮЗЕРНЕЙМ питер POSEMOJI мем ещё сероквель пару недель ждали выяснилось нигде заказу не позвонили ща ждём квентиапин четвёртый дохера аптек обшарили\n",
      "юзернейм питер POSEMOJI мем ещ сероквел пар недел ждал выясн нигд заказ не позвон ща ждем квентиапин четверт дохер аптек обшар \n",
      "\n",
      "зодак цитрин фексадин кларитин ксизал супрастин лоратадин фликсоназе не помогает попробую эриус\n",
      "зодак цитрин фексадин кларитин ксиза супрастин лоратадин фликсоназ не помога попроб эриус \n",
      "\n",
      "каждый новыми таблетками зовут паксил надеюсь оправдаешь своё название дашь пак сил паксил держи трясучечку\n",
      "кажд нов таблетк зовут пакс над оправда сво назван даш пак сил пакс держ трясучечк \n",
      "\n",
      "доброе утро коллеги кофеин состоит углерода водорода азота кислорода кокаин талидомид нейлон тротил героин приятного кофепития ЮРЛ\n",
      "добр утр коллег кофеин состо углерод водород азот кислород кокаин талидомид нейлон трот героин приятн кофепит юрл \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in res:\n",
    "    print(tweet)\n",
    "    print(' '.join([stemmer.stem(w) for w in tweet.split(' ')]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "mstm = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:11<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "lem = []\n",
    "for tweet in tqdm(res):\n",
    "    lem.append(''.join(mstm.lemmatize(tweet)).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['слюнтяй утверждать суицид удел слабый человек находить тысяча причина глотать горсть прозак хватать смелость',\n",
       " 'лыжа сальбутамол помогать россия норвегия',\n",
       " 'оланзапина помогать назначать галоперидол хотеть докатываться галоперидол делать проклятый тревожность',\n",
       " 'флуоксетин катастрофически хотеться почему блин теремок влезать теремок спасибо живой',\n",
       " 'мой голова хенвон любить lil peep перкосать молль перпл дранк наркота курс организовывать нелегальный вечеринка',\n",
       " 'ципрофлоксацин смысл жизнь',\n",
       " 'ветеринар прописывать ингаляция фликсотид мгк раз сутки прописывать вентолина мгк случай приступ',\n",
       " 'день выпивать таблетка алпразолам чувствовать расслабленно ох день час спать',\n",
       " 'юзернейм лечиться футболист астма синдром дефицит внимание столь любимый нероссийский спортсмен законный амфетамин декстроамфетамин',\n",
       " 'юзернейм порекомендовать сотрудник wada выпивать мельдония вдыхать сальбутамол попробовать приближаться результат любой медалист splits вад крушельницкий',\n",
       " 'юзернейм питер мема еще сероквель пара неделя ждать выясняться нигде заказ позвонить ждать квентиапина четвертый дохер аптека обшаривать',\n",
       " 'зодак цитрин фексадин кларитин ксизал супрастин лоратадин фликсоназ помогать попробовать эриус',\n",
       " 'каждый новый таблетка звать паксил надеяться оправдывать свой название давать пак сила паксить держать трясучечка',\n",
       " 'добрый утро коллега кофеин состоять углерод водород азот кислород кокаин талидомид нейлон тротил героин приятный кофепитие']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pymorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полный скрипт препроцессинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(out_file, prep = True, lemmatize = None):\n",
    "    with open('../Data/task2_ru_training.tsv', 'r', encoding='utf-8') as f:\n",
    "        df = pd.read_csv(f, sep=\"\\t\")\n",
    "    \n",
    "    if prep:\n",
    "        new_tweets = []\n",
    "        for tweet in tqdm(df['tweet']):\n",
    "            new_tweets.append(process(tweet, log=False, lang = 'rus'))\n",
    "        df['tweet'] = new_tweets\n",
    "        \n",
    "    if lemmatize == 'mystem':\n",
    "        lem = []\n",
    "        for tweet in tqdm(df['tweet']):\n",
    "            lem.append(''.join(mstm.lemmatize(tweet)).strip())\n",
    "        df['tweet'] = lem\n",
    "    elif lemmatize == 'snowball':\n",
    "        lem = []\n",
    "        for tweet in tqdm(df['tweet']):\n",
    "            lem.append(' '.join([stemmer.stem(w).strip() for w in tweet.split(' ')]))\n",
    "        df['tweet'] = lem\n",
    "    elif lemmatize == 'pymorphy':\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    with open('../Data/{}'.format(out_file), 'w', encoding='utf-8') as output:\n",
    "        df.to_csv(output, sep='\\t', index=False, line_terminator='\\n')\n",
    "\n",
    "    print('Done! Your data is in {}'.format(out_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'snowball_lem.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6090/6090 [00:27<00:00, 220.24it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 6090/6090 [00:08<00:00, 739.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Your data is in snowball_lem.tsv\n"
     ]
    }
   ],
   "source": [
    "write_data(out_file, prep = True, lemmatize = 'snowball')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
